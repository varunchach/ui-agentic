# LLM Configuration - LiteLLM (Custom Endpoint)
# Using LiteLLM endpoint
CUSTOM_LLM_ENDPOINT=https://your-litellm-endpoint.com/v1
CUSTOM_LLM_API_KEY=your_litellm_api_key_here
CUSTOM_LLM_MODEL=your-model-name
CUSTOM_LLM_TEMPERATURE=0.0

# Alternative: OpenAI
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL=gpt-4-turbo-preview
# OPENAI_TEMPERATURE=0.0

# Alternative: Anthropic
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_MODEL=claude-3-opus-20240229
# ANTHROPIC_TEMPERATURE=0.0

# Alternative: Azure OpenAI
# AZURE_OPENAI_API_KEY=your_azure_key_here
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name
# AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Embedding Model
NOMIC_EMBED_MODEL=nomic-embed-text-v1.5
EMBEDDING_DIMENSION=768

# Vector Database
FAISS_INDEX_PATH=./vector_store/faiss_index
VECTOR_STORE_PATH=./vector_store

# Re-ranker Configuration
BGE_RERANKER_MODEL=BAAI/bge-large-en-v1.5
RERANKER_TOP_K=10

# Chunking Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
ENABLE_SECTION_AWARE_CHUNKING=true

# Retrieval Configuration
RETRIEVAL_TOP_K=20
RERANK_TOP_K=5

# Application Settings
LOG_LEVEL=INFO
MAX_FILE_SIZE_MB=50

# Optional: Hugging Face Token (for better rate limits when downloading BGE re-ranker model)
# Not required for public models, but recommended for reliability
# HUGGINGFACE_API_TOKEN=your_hf_token_here

# Embedding Model (faster alternative to nomic)
# Options:
# - BAAI/bge-small-en-v1.5 (fast, 384 dim) - RECOMMENDED
# - sentence-transformers/all-MiniLM-L6-v2 (fastest, 384 dim)
# - sentence-transformers/all-mpnet-base-v2 (balanced, 768 dim)
# - nomic-ai/nomic-embed-text-v1.5 (slower but high quality, 768 dim)
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
EMBEDDING_DIMENSION=384

# LLM Optimizations (KV-Caching & Speculative Decoding)
# Enable/disable optimizations
LLM_OPTIMIZATION_ENABLED=True

# KV-Caching: Caches key-value pairs from previous tokens (speeds up generation)
# Usually automatic in modern LLMs, but can be explicitly enabled
KV_CACHE_ENABLED=True

# Speculative Decoding: Uses smaller draft model + larger target model
# Can improve throughput by 2-3x while maintaining quality
# Requires a smaller/faster draft model available on your LiteLLM endpoint
SPECULATIVE_DECODING_ENABLED=true  # Enabled by default
SPECULATIVE_MODEL=your-draft-model-name  # Example: smaller model for drafting

# Tool Configuration (Optional - for enhanced capabilities)
# Web Search - Choose provider
WEB_SEARCH_PROVIDER=duckduckgo  # Options: duckduckgo, serpapi
# SERPAPI_API_KEY=your_serpapi_key_here  # Optional: for SerpAPI provider

# Economic Data APIs (Optional)
# FRED_API_KEY=your_fred_api_key_here  # For Federal Reserve Economic Data
# GDP_API_KEY=your_gdp_api_key_here  # Alternative GDP data source

# Web Search - Tavily (Recommended for RAG)
# Get API key from: https://tavily.com/
TAVILY_API_KEY=your_tavily_api_key_here
WEB_SEARCH_PROVIDER=tavily  # Options: tavily, duckduckgo, serpapi

# Alternative: SerpAPI (if not using Tavily)
# SERPAPI_API_KEY=your_serpapi_key_here
